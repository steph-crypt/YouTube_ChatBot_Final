because we have perhaps stumbled in the
past that's no reason to think we need
to stumble in the future hold on to your
Technical and moral imaginations and we
will make progress you made me you know
king of the world we limit AIS to being
tools only tools to help humans solve
human problems and we do not give them
agency we do not allow them to take over
you know large systems we need
technology to keep them limited and
that's what I'm thinking a lot about
right now autonomous systems AI man with
ey patch we think you're training for
the next Bond villain that's very
appropriate for the topic we're going to
[Music]
discuss this is Star Talk special
edition Neil degrass Tyson you're a
personal astrophysicist and when we say
special edition it means I've got as
co-host Gary O'Reilly Gary hi Neil all
right Chuck nice baby hey hey what's up
Ma so I understand today because our
special edition themes are always people
our physiology our Behavior our conduct
our interaction with the world and
finally we're talking about Technologies
being safe and ethical yeah these are
three words you don't often see in a
sentence safe ethical technology and
this is where we're going to shine our
light where you going to take us today
it seems we have problems starting with
the letter A algorithms AI autonomous
well there's three for you is there a
wild West Tech bubble in play right now
one with no guard rails no moral compass
that's run by wannabe Bond villains are
the best of human values baked into
Technologies during the design process
is there anyone working on ethical and
safe operating protocols for these new
technologies the answer is yes and we
will meet two people responsible shortly
courtesy of the future of Life Institute
that has this year acknowledged their
work previous honores include KL San for
popularizing the science of the nuclear
winter and our first guest who follows
shortly is baa fredman so baa Friedman
welcome to Star Talk well thank you yeah
if I have my my my data here is correct
on you professor at University of
Washington's information school udub I
think you guys call it is that correct
that's right yep but you're also co-
founder of the value sensitive Design
Lab ooh you're thinking about The Human
Condition You focus on the integration
of human values and ethics with new
technologies that are being born very
important don't come to it when it's too
late yeah when we're all extinct is it
maybe we should have done that different
right exactly yeah when the robots are
like how do you like me now that's a
little late so so baa please explain the
focus of your value sensitive Design Lab
yeah sure um you know I started out uh
as a software engineer along long long
time ago and I wanted to build
technologies that worked and were
efficient and effective but I also
wanted some confidence that they would
do something good in the world you know
that whatever I made as an engineer
would ultimately benefit um Society
human beings other creatures on the
planet design constraints are our
friends
they um they help us shape the kinds of
new technologies we develop and their
qualities and characteristics in ways
that um maybe we want to see and so I
think of design constraints as trying to
bring together our moral imaginations
and our technical imaginations and that
leads to really great engineering design
you know so if I think about Energy
Technologies I want Energy Technologies
that will give us lots of power that
will do so in a way that is consistent
with how the rest of the biology and
Planet
functions and um and has limited risk in
terms of generating um waste or too much
power so if I give myself those design
constraints you know as an engineer as
somebody who's developing new materials
I start looking at what kinds of sources
for energy I might want to evolve like I
look a lot at chlorophyll and I just
think how remarkable is this all these
green things somehow manage to absorb
energy that's out there from the sun
it's there and then transform it into a
way in which it can be used that seems
like a really great idea and there isn't
a lot of waste generated that lays
around and is dangerous to us for
thousands if not tens of thousands of
years well technically there is a waste
product it's called
oygen not such a bad waste product for
us H that's the trees waste product is
is oxygen yeah but that's the way that a
design constraint that brings together
our moral and Technical imaginations can
lead us in I think yeah new and Powerful
directions do you try to consider
unintended consequences of design or is
that part of the process or does it just
oh well they were unintended it was
unintended why do you think they call
them unintended
that is such an important question so
you know let's be honest anything we
design and put out into the world um we
put out into the world and people are
going to do stuff with it and they're
going to do things with it that we
didn't anticipate like the telephone is
a great example the telephone was never
expected to be this communication device
that people used in their homes and it
connected women who were staying at home
and and created a whole Society for them
that was an unintended consequence
interesting or the cookies that are
being used on your computer right now
those are a complete unintended
consequence that was just a little bit
of data that was left on your machine to
help debugging when browsers were first
being developed when that protocol was
first being developed wow and its more
massive impact has been our experience
with cookies now so yeah what's the
takeaway we design with our eyes open
and then after we deploy something we
keep our eyes open and we hold ourselves
accountable for what happens as people
take up these Technologies and use them
so the design process goes longer than
oh I had my big release the design
process follows it out and when we see
new things emerge we're proactive we're
alert we're proactive and we see that as
as part of our responsibility as the
technologists and Engineers so allow me
to push back on you just a little bit
here first let me agree of course any
good engineer loves constraints because
that's the test of their Ingenuity and
creativity okay if they say do it for
this much money with this much energy
fit it into this volume that's how you
get Discovery okay that's how we folded
up the James web Space Telescope into a
rocket fairing some engineer said what I
got to put a 8ft telescope I would have
the diameters into this time tiny
Fairing and they go home and come back
and figure out how to do it un furls
like the Petals of a flower right so
we're all in on that however I let me
just push back here and say if I'm in
the lab about to invent something that
could be highly useful to society or
possibly even destructive but it's just
a discovery of the science embedded in
some bit of engineering why should it be
my responsibility to design it how you
want me to rather than your
responsibility to convince people how to
use it ethically I can invent a knife is
there an ethical knife I don't know but
we want to train people how to use
knives or any bit of technology and any
tool that comes out of the brainchild of
scientists put into play in the
brainchild of of of Engineers so I don't
know that your constraints in my lab are
the right thing for me when I just want
the freedom to explore and discover and
let the ethical invocations happen after
the fact and baa now you know why
scientists are going to kill us all no
stop well I'm just Neil I'm just going
to Mark a word in in your comment which
is the word you like who who is the you
here which you and and and how should we
think about those different use and some
of the things I think about when I do
think about this question I think
there's um discovery of basic knowledge
like fundamental underlying phenomena of
the universe we split the atom that was
basic knowledge and I see that as a
different Enterprise than the
engineering Enterprise of tools and
technologies that we're going to deploy
in society I'm with you then so I am a
strong proponent of um very very diverse
scientific exploration in fact I
actually would claim that you know as a
country in the United States our um
scientific exploration is far more
narrow than what I would like to see and
I would really push hard so based on
what you just said all right here's an
ethical question there's a scientist who
discovers a cure for cancer using a
virus that can easily be
manipulated as a biochemical weapon that
could destroy an entire country in the
course of 48 hours this is the most
virulent organism that's ever been
placed on
Earth would you say go ahead and make
that right I'm gonna hold on to that for
a minute and I'm just going to go back
to Neil's comment and then I'll I'll
return to that okay because I also want
to say to Neil's comment you know we
have limited um time and resources so it
is always the circumstance that we are
choosing to do some things and not do
other things so it's it's really a
choice of where am I going to direct my
time and energy where am I going to um
place my imaginative energies and
Innovation and which ones am I not going
to do right and we saw in the 80s for
example a real push of resources towards
the development of nuclear energy and
away from
photovoltaics right so we live in that
kind of resource I don't know if you
would call it resource scarce but at
least we don't get to work on everything
at full force all at the same time and
we have to recognize we are making
choices so one of the first things I
would say is um how do we make really
good choices there how do we use the
resources we have in a way that will be
most constructive for us my own gestal
on that is on the basic science side of
things I say spread those resources
across a wide diversity of different
kinds of Science and um different kinds
of ideas far more diverse than what I
think we tend to do in the United States
and on the engineering side and now
maybe I shift bat Chuck to your question
which is really a great question I don't
tend to see the world in terms of forc
choices or design tradeoffs in the way
that you framed it I want to bring back
in that notion of constraint and I want
to bring back in that notion of
imagination I think likely enough if we
understand something about um whatever
this biology is or whatever the piece is
that might be a prevention against
cancer that if we push hard enough on
ourselves we will be able to invent ways
that use that knowledge without having
to also risk a really deadly virus and I
think the propensity to
say it's a tradeoff it's X or
Y we really limit ourselves in our
abilities so in the work that we do we
have moved away from the language of
design tradeoff or value conflict and we
talk about tensions and then we talk
about how you resolve
tensions and we talk about trying to
populate this space with a whole range
of better Solutions they're not necessar
neily Perfect Solutions they're better
Solutions and so that would be the
approach I would take now I don't know
that science to know where how it might
go but that would be my my intuition
brilant great great answer and
insightful and great that's actually
been done many times before uh just it's
a slight tangent to your line of work
but it's it's related when they used to
do crash tests right with pigs right
because you can get a hog that has sort
of same sort of body mass as a human put
him in the driver's seat crash the car
and the hog dies plus it's the nearest
thing to human skin yeah yeah okay so
the hog dies and say well there's no
other way to do this you might say at
the time until you say no think of
another way and then we have the crash
test dummy yeah and the crash test dumm
is even better than the because you can
put sensors everywhere throughout and so
that's that so I I agree not perfect but
better yeah yes
or or even maybe better and perfect
right I I agree that it's a false choice
to say I can only do this if I possibly
set loose a virus that kills an entire
country well then maybe you're not
clever enough and keep at it I was
clever enough to kill a whole country no
I'm
joking okay I have one last thing and
before we wrap one last thing earlier
you said you'd want the ethical Compass
to be pointed in a direction that serves
us serves civilization in some way if we
were 70 years ago in the American
South
the ethical Compass was oh let's create
something where we can get more work out
of the slaves and then we all benefit
that would be the ethical Compass
working in that time and in that place
so what confidence do you have that
whatever line of ethic whatever ethical
Direction you want to take something in
the room with the inventors that that's
will still be the ethics that we value 5
years later 50 years later 100 years
later so it's a great a really great
question I'm going to answer it in a
couple different ways the first thing
that I want to remind us all about is
that you know moral philosophers have
been trying to identify a workable
ethical theory that cuts across all
situations all times and we have some
really good ideas but none of them cover
all the situations that our intuitions
tell us about so sometimes a
consequentialist theory is good but it
comes up short and then there's a rights
based Theory but it comes up short we
can go to Buddhist ethics we can go to
Islamic ethics we can um go
to various you know various ways of
thinking so this the place where we are
that we just have to accept is that um
while we're waiting to figure that out
from a conceptual ethical moral point of
view we still live in the world and we
still need to act in the world and so
the work that I've done has tried to
take that really seriously to create a
space for ethical Theory without um
explicitly saying which ethical Theory
and also leaving room for as we learn
more that we can bring that in so that's
a little background to what you're
saying so now what does value sensitive
design do for the circumstance you're
talking about it puts a Line in the Sand
and says you have to engage with all
stakeholders direct and indirect who are
going to be implicated by your
technology that means that not only do
the people who want to benefit from
somebody else's labor not only are they
stakeholders but those people who are
laboring are stakeholders and value
sense of design says they are legitimate
stakeholders and their views come into
the design process without giving more
power to one than another that's
incredible that's highly enlightened
where were you 170 years ago
right so these are these are about
practice this is about practice and
about implementing these practice and so
I'm going to tell you a story about a
project a very particular project and
you'll see why and how this actually
matters and is practical it's not pie in
the sky so in the state of Washington
where I live there is something called
the access to Justice technology
principles that govern how the courts um
give access to technology what they are
required to do and they were first
developed maybe 15 years ago 20 years
ago and then they wanted to update
them and the committee that updated them
came to my lab and they said you know
we've done a good job updating them but
we don't feel like we've really reached
out to diverse groups of people can you
help
us my lab developed a method called the
diverse voices process for Tech policy
and the idea is that you know the hits
the road with the words on the page so
if we take a tech
policy in its sort of polished draft
form and we can let groups that might
otherwise be marginalized scrutinize
that language give feedback and then we
can help change those policies
responsive to them then we can improve
things so we did we ran panels with
people who were formally incarcerated we
ran them with uh immigrants we ran them
with people in rural communities and we
actually ran them with um the people who
do the court administration because
they're also really key
stakeholders as a result of the work we
did there were two principles that were
surfaced one was about human touch and
the other was about language and people
said to us things like look if somebody
is going to deny me parole and I'm not
going to get to be there for my kids
13th birthday or hang out with them at
at you know their soccer games you can
relate to that right Gary thank you um I
want a human being to look me in the eye
and tell me that that's what my life is
going to be for the next year because my
peole is denied I don't want to hear
that from an AI I don't want to hear
that from uh a piece of technology I
want a human being to tell me that
because this is a human experience right
and so so in fact we gave that feedback
back to the committee the committee then
added in new principles actually around
human touch and those were approved by
the Washington state supreme court a
couple of years ago and those access to
technology principles are a model that
many states in the United States follow
so what I'm talking about is really
practical and we're talking about how we
actually improve things in practice be
it on the technology design side or on
the policy side that governs how the
technology is used and I love the fact
that a state can do that independently
from the federal government and be so
good at it or so emulat that other
states will then use that as the model
and then that can spread across the
country with or without Federal guidance
on top of it yeah excellent well I guess
if I was going to say one one last thing
it's you know because we have perhaps
stumbled in the past
that's no reason to think we need to
stumble in the future or stumble in the
same way you know so really my takeaway
to everyone would be hold on to your
Technical and moral imaginations and
hold yourselves and your friends and
your colleagues um and the technology
you buy accountable to that and we will
make progress some incremental and some
perhaps much bigger but that as a as a
keystone I think is is really good
guidance for us all a reminder why you
are this year's winner the future of
Life Of
War congratulations thank you for being
on Star Talk your vision for us all uh
gives us hope which we need a lot of
that right now absolutely okay thank you
and let me just say B you as a thank you
as an avid lover of alcohol I have
stumbled in the past and I am sure to
stumble in the future as well do we need
to end on that note chck
okay some things we can ignore okay we
wanted to take this break to
congratulate the winners of this year's
future of Life award for their
significant contributions and
advancements in AI safety and computer
ethics the winners are James Moore
Bacher Friedman and Steve omah hundro
who each embody the most crucial
elements needed to ensure that safety
and ethics are embedded in Ai and
emerging Technologies to learn more
about future of Life Institute and this
winners make sure to visit futureof
life.org next up our next future of Life
Award winner Steve omohundro yes yes he
he's thinks about AI there's not enough
of not like I think not the way I think
about yes there's not enough it seems to
me there's not whatever he did there's
not enough of them in the world I
believe if we're thinking about the
ethics of AI That's on everybody's mind
right now I mean for Steve welcome to
Star talk thank you very much yeah so uh
for those who can see this on video uh
you're Dawning an ey patch and you said
you had recent surgery but none of us
none of us believe we we're not buying
it we think you're training for the next
Bond villain yes that's that's very
appropriate for the topic we're going to
discuss you know autonomous systems AI
mid ey patch ouch W equals Bond villain
there's the equation so where are we now
with establishing AI ethics because the
AI it's it's Delights some people myself
included it freaks out other people and
we're all at some level thinking about
the ethical invocation of it before AI
becomes our Overlord so what what is the
current status of that right
now well I think we're right on the edge
of some very important developments and
very important human decisions uh I've
been working working in AI for 40 years
and for the first half of that I thought
AI was an unabashed good we'd cure
cancer we'd you know solve Fusion all
the basic human problems we would solve
with AI but then about 20 years ago I
started thinking more deeply about well
what's this actually going to happen
when if we succeed what's going to
happen when AIS can really reason about
what they want to do and I discovered
that um there are these things I call
the basic AI drives uh which are things
that basically any AI which has uh
simple goals and I used to think about
Chess playing AIS uh will want to do and
some of those are get more resources so
it can do more of what it wants to do
make copies of itself keep itself from
being turned off or changed and so those
things in the context of the human world
are very risky and very dangerous uh we
didn't have the AIS 20 years ago that
could do that but we're about to have
those in the next probably year or two
so this is a critical moment for
Humanity I would say so where do you
stand on this on the subject of
Consciousness engineering those that
want to engineer AI for Consciousness
and those that want to not what's the
benefit the good or bad here is that the
difference between a blunt computer that
serves our needs and one that thinks
about the problems you are the
self-improving algorithms all sort of
things like that yeah exactly well I
think long term you know we we may very
well want to go there in the short term
I think we're nowhere close to being
able to handle that kind of a system so
I would say you know if you made me you
know king of the world uh we limit Kurt
AIS to being tools only tools to help
humans solve human problems and we do
not give them agency we do not allow
them to take over you know large systems
it's not easy necessarily to do that
because many of these systems will want
to take over things and so we need
technology to keep them limited and
that's what what I'm thinking a lot
about right now in my field it's exactly
I mean we've been enjoying AI for a long
long for a long time and it's been a
tool it's not and a brilliant beautiful
tool makes our lives easier uh and uh
once they're trained we we go to the
beach while it does the work and I'm
good with that uh but yeah we're not
working with AI with agency yeah because
then it would be like so how was the
beach no that's AI with attitude I hope
you enjoyed
yourself while I was here slaving away
over my calculations AI with Attitude
yeah so if if we do have ai with agency
and then we continue to use it as just a
toll do we not get legal on the phone
and all of a sudden we're into contracts
and oh yeah big problems you know can
they vote can they own property right
and the latest models are have been
discovered that they do do they do
something called siop Fancy which is
they're trained to try and give
responses that people rate as good
well AI very quickly discover that if
you say that was a brilliant question
you must be an amazing person then
people say yeah that was a really good
response and so they'll just make up all
kinds of stuff like that so they're
ass-kissing well they know that we love
that exactly so where does it stand now
today is there a table you should be
sitting at where you're not as we go
forward on this Frontier yeah I mean so
so who's going to make these decisions
well it has to be somebody who
understands the technology who
understands the technology the companies
do and so open AI Deep Mind anthropic
and the Elon Musk xai is sort of an
emerging one these are the companies
that are building these systems at the
Leading Edge they call them Frontier
models and because they're the ones who
know what's going on they're the ones
making these decisions now the
government has recently realized oh my
goodness we better get involved with
this and so there have been a lot of
Partnerships announced over the last few
few months actually between governmental
agencies intelligence agencies defense
agencies and these Leading Edge AI
companies and so I think some kind of a
new combination is emerging out of that
that's going to make the actual end end
decisions so how do you incentivize
these these tech companies to embrace
the safety architecture and not go
gung-ho and disappear off on their own
agendas that is the Big Challenge and if
we look at the history of open AI it's a
little bit of a cautionary tale it was
created I think around 2017 in response
to Google's Deep Mind which was making
great progress at the time right and a
group of people said oh my God we really
have to worry about AI safety it looks
like this is happening quickly let's
start a special company which is
nonprofit and which is particularly
focused on safety and they did that and
everything was great Elon Musk was one
of the forces behind it there were
internal struggles and so on and musk
left well when he left he took away some
of the money he was going to give them
so then they decided oh we need to make
money and so then they started becoming
more commercial and that process has
continued a group of the researchers
there said wait a minute you're not
focusing on safety they left open aai
and they started in thropic to be even
more safety oriented and now anthropic
is also becoming much more commercial
and so the forces the commercial forces
the political forces the military forces
they all push in the direction of moving
faster
and you know get more advanced more
quickly whereas the safety everybody
wants safety but they sort of compete
against the these economic and political
forces I was in the U UAE a couple of
years ago and if I remember correctly
they have a minister of AI and as does
China and some other uh countries sort
of emergent on this space how do we get
that kind of ear and audience within our
own governmental system the military
does have an AI group that's about this
absolutely as you would want them to
yeah but in terms of policy and laws and
legislation do we need a a Cabinet
member who's Secretary of AI or
Secretary of computing something some
structural change yeah oh this is the
biggest change to humanity and to the
planet ever and it looks like it's
happening you know sometime over the
next decade and many are predicting very
short timelines and so we as a species
humanity is not ready for this and so
how do we deal with it and people many
people are starting to wake up to that
fact and so there are lots and lots of
meetings and organizations and groups um
it's still pretty incoherent I would say
so Steve if we if you've got this
talking shop going on where something
may or may not get done are we misplaced
focusing exactly on AI when we've still
got Quantum Computing on the horizon oo
good one yeah are we how much of this is
premature but won't they go hand in hand
so it's like whatever whatever problems
you have with AI and whatever
considerations you're making with AI
you're just going to have to transfer
them over to Quantum Magi so you should
really start dealing with it now but if
you're not in at the ground floor not in
a tool well let's just Steve hit this go
ahead Steve well so I'll give you an
example so Quantum computing you know if
it were successful would break much of
the public key cryptography that's used
in the world today and so nist has been
busily trying to create a postquantum
cryptography where they create new
algorithms which wouldn't be vulnerable
to to Quantum Computing but uh meta for
example has a group which is using the
latest AI models to break these
postquantum uh algorithms and they've
been successful at breaking some of them
and so like you say the two are going
hand inand AIS will be much better at
creating Quantum algorithms than humans
are and that may lead to some great
advances it may also lead to you know
current cryptography not not
withstanding that and so uh that's
another whole wave of of uh
transformation that's likely to happen
we we just make every password 1 two 3 4
no AI would ever go for that they' be
like oh yes so
ridiculous and and listening to you
Steve it it reminds me was it Kurt vaget
in one of his stories I don't remember
which he said these are the last words
ever spoken in the human species yes two
scientists saying let's try it this
other
way that's the end yeah there you go and
that was it
yeah let's let's try AI in this other
mode right boom that's the end of the
world right right there so you can set
ethical guidelines but that doesn't stop
Bad actors out there no that means a bad
actor can take over the world while the
rest of us are obeying ethical guides so
what are the guard rails put in place
place for something like that I think
that's one of the greatest challenges we
now have open source language models
open source AIS that are almost as
powerful as the the ones in the labs and
far more dangerous and uh and they're
being downloaded hundreds of millions of
times and so you have to assume every
actor in the world now uh China is now
using uh meta's latest models for their
military Ai and so I believe we need
Hardware uh controls to limit the the
capability of uh so right now the
biggest AIS require these gpus that are
quite expensive and quite large the the
latest one is the Nvidia h100 it's about
$30,000 for a chip yeah the US uh put an
embargo on selling those to China but
apparently China has found ways to get
the chips anyway um people are gathering
up these chips Gathering huge amounts of
money hundreds of well certainly
hundreds of millions of dollars billions
of dollars and now they're even talking
about trillion doll data set over the
next few years and so the good news is
if it really costs a trillion dollars to
build the system that will host the
super duper AI then very few actors can
uh pay that and therefore it'll be
limited in in its extent you just
described where the next Frontier of
warfare will exist yeah absolutely
absolutely one thing you know it's
pretty obvious these data centers are
going to be a Target and they don't see
to be building them in a very hardened
way so I think that's something people
need just start thinking about maybe
underground data centers Steve are
we looking at something in terms of the
safety aspect here that's doable or are
we just the kings of wishful thinking I
want to make sure we got the good the
good thoughts here yeah because I I
don't want to leave this conversation
with you completely buming us out okay I
hope not to do that yeah yeah Steve give
us a place where we can say thank you
Steve for being on our show and be able
to sleep tonight yeah the truly safe
technology needs to be based on the laws
of physics and the law and the
mathematical proof those are the only
two things that we can be absolutely
sure can't be subverted by a
sufficiently powerful Ai and um AIS are
getting very good at both of those
they're becoming able to model physical
systems and design physical systems with
whatever characteristics we want uh and
they're also able to uh perform
mathematical proof in a very good way
and so it looks to me like we can design
Hardware that puts constru on AIS um of
whatever form we want but that we need
AI to design this hardware and that if
we can shift Humanity's technological
infrastructure you say AI please design
your own prison cell that we're going to
put you in that's what you just said
exactly it's all and then it's going to
design way you certainly don't want an
agent to do that because then they'll
find some way to hide a backd door or
something but by using mathematical
proof we can get absolute guarantees of
about the properties of systems and
we're just on the verge of that kind of
technology and so I'm very hopeful that
probably the next two or three years uh
you know there are several groups who
are building um superhuman
mathematicians uh and they're expecting
to be at the level of say human graduate
students and Mathematics by the end of
this year using those AIS uh we can
build designs for systems that have
properties that we are very very
confident in and so I think that's where
real safety is going to come from uh but
it builds on top of AI so we need them
both I was going to say the good thing
about what you just said even though it
sounds crazy to have the inmate design
its own cell is that without agency at
this point it's just a drone carrying
out an order yeah so that's that's the
encouraging part you know so yeah
whereas if it were ceni in any way or if
it had some kind of agency it could very
well say yeah I'm also going to design a
back door and the trap door and I'm not
going to tell you and I'm not going to
tell you Steve I first congratulations
on winning this award you are exactly
the right kind of person who deserves
such an award that gives us hope for the
future of our relationship with
technology yeah yes and the health and
wealth and security of civilization as
we go forward yeah so thank you so much
and I look forward to the day where an
AI beats you out for this award oh right
yeah we want that's a great point maybe
next year it'll be an AI that
wi I'm joking by the way Steve omah
handro winner of this year's future of
Life award award and deservedly so thank
you thank you very much before we jump
to our next segment I need to
acknowledge the third Honore James Moore
who is now sadly deceased his paper in
1985 what is computer ethics established
him as a pioneering theoretician in this
field his policy vacuum concept created
the guidelines to address the challenges
of emerging Technologies his work
profoundly influencing today's policy
makers and researchers gone but not
forgotten
so seems to me that for ethical
principles to work at all they have to
be everywhere at all times and capable
of evolving with the technology self I I
can't foresee the the ethics panel
getting together and from on high
declaring what is ethical and what isn't
and then everyone has to obey that for
the next 10 years you put 10 people in a
room near you get 12 opinions right
that's a basic human nature then you've
got to get all of these components all
of these nation states or whatever
investment groups demographics
demographics with their own agendas to
buy in into the same principles because
on a Wednesday the principles not the
same for them they're going to think in
a different direction oh that doesn't
that doesn't even scare me what scares
me more than anything China Russia North
Korea yeah it's that s serious I'm not
even going this is China Russia North
Korea we can put any constraints we have
on ourselves there you go doesn't mean
anybody else is paying attention and
that's the problem and you're Hing cats
good luck yeah that's well that's what
makes it so scary well well hurting cats
with nuclear yeah yeah we're hurting
nuclear cats hurting nuclear exploding
nuclear cats exping nuclear cat it's the
newest it's the newest game sweep in the
internet gives a whole other meaning to
shortener
cat dead or alive I mean you've got
autonomous systems and if it's geared to
say if it's human kill
it problematic here's another little
known fact when we signed with the
Soviet Union the nuclear Test Ban Treaty
mhm that was progress
this was you will no longer test nuclear
weapons because at the time from the
late 1950s into the early 1960s there
several tests a day in some in some
years there several tests a day okay
somewhere in the world right and which
made for such great video okay so we
said so we we so this has to stop all
right yeah what is a little known fact
and we write about this in the
accessory to war the unspoken alliance
between astrophysics and the military
book in that book we highlight the fact
that we agree to that around the same
time that computing power was good
enough to calculate the results of what
would be a
test so we didn't really stop
testing not philosophically not morally
mhm was that where mad came from
mutually assured destruction oh that was
later that was later I'm not convinced
based on my read of history that anyone
any one nation can util say oh we're
going to just do nice things and moral
and ethical things with this new
technology right I I don't yes let's say
you do that but no one else does it then
what difference does it make what
difference does it make you know you got
to play by the same rule book but we
know that's not likely to happen yeah I
mean what was history of our species is
offers great evidence for that
impossibility but when you when you
listen to baa talking there's such a
strength in the points that she makes
you would hope that would people will go
you know what yeah and the majority come
online and then these guys sit in
isolation testing you know
intercontinental ballistic missiles but
but the Mad concept Mutual assured
destruction just think about that that
brought the United States and the Soviet
Union to the table yes not because they
thought nuclear weapons were bad but
they realized they couldn't win right
right and that's the problem the war
when you can't win now and what that
does that doesn't mean they didn't
wern't thinking about it or or if they
could win they would and it also doesn't
mean that they've been taken into
account for what I call the Nero
scenario what's that so what did Nero do
he fiddle while Rome burned he did he
burn it down he didn't care so what
happens if you know you're still in a
position where the danger is ever
presentent so just speak because I spent
enough time hanging around military
people I don't talk about I'm not
talking about hawks that you know that
just want I'm just talking about people
who think about the history of conflict
in this world the behavior of other
members of our species not just one guy
standing there going do you smell that
son that smell do you smell it I know
where that came from
yeah the apocalypse but you see
Apocalypse Now yeah you speak to the
generals and the majors you find
invariably they're students of War
they've understood strategies they
understood histories the provocations
and the outcomes and most of them are
not the war mongers we stereotype them
to be exact because of that knowledge
that understanding correct and so I just
I I don't have the confidence I mean I
wish I was I was as hopeful as baa yes
is I I want to be that hopeful I will
aspire to be that hopeful so I just
wonder when when she talks how far ahead
of a story in terms of a Technology's
development are they and how far are
they playing catchup and you know are
they being able to bake it in from the
get-go or are they just trying to Retro
engineer what's gone wrong it could be a
new emerging philosophy where everyone
knows to bake it in from the beginning
that would be a shift in our conduct
awareness the kind of shift for example
dare I harp on this yet again that when
we went to the moon to explore the moon
we looked back and discovered Earth for
the first time
yes AC around the world people started
thinking about Earth as a planet Earth
as a holistic entity that has
interdependent elements there's no one
Island distinct from the rest anything
else that's going on on this planet
There's No Boundaries No Boundaries we
share the same air molecules water
molecules and that was a a firmware
upgrade to our sensibilities of our
relationship with nature and that's why
to this day people all around the world
say we got to save Earth and nobody was
saying that before we went to the moon
and looked at Earth in the sky right all
the peace Nicks at the time in the 1960s
they was just anti-war they weren't
let's save the Earth nobody had that
kind of sensibility so maybe it's a
sensibility upgrade that's waiting to
happen on civilization lest we all die
at the hands of our
own discoveries yeah I'm I'm going with
the last
part I'm just saying that you know uh
you talk about Earth Day you talk about
we went to the moon and there are people
who think we didn't go to the moon and
that the Earth is flat yeah we are we're
screwed and by the Earth Day the first
Earth day was 1970 right while we were
going to the moon and the irony 1960 but
it wasn't no might have been delayed
1980 no while we were going to the Moon
first Earth Day right so is the irony
that we lean into AI to get it to help
us create ethical and safety
architecture help it save us from
ourselves I like that maybe that's the
way to to flip the table right yeah and
that should be it and say AI they're bad
actors among humans who are trying to
use AI to get rid of humans Now kill
them
[Music]
CH first thought this is where they
live they dress docks them this this is
their daily
routine Google knows your daily routine
we really are Android knows you've been
Googling knows everything we really are
bad people really are bad so maybe it's
the good AI yes against that's the
future battle good AI against versus bad
a evil EV
evil AI but then again the bad The Bad
The Bad AI will tell you that the good I
is the bad Ai and then you the first the
first casualty of War right is always
the
truth oo yeah thank you I don't know
Brant that was deep yeah yep oh that's
deep and truthful I wish it weren't true
exactly stop speaking the truth won't
you lie to us every now and then yeah
like all like everybody else got do you
could be get me a new
program all right this has been our
future of Life installment of Star Talk
special edition yeah yeah I enjoyed this
thank and congratulations to the award
winners they they are the people what we
need out there yes L we not be around to
even think about that problem AB first
place all right Gary Chuck pleasure
always good to have you Neil degrass
Tyson here as always bidding you we keep
looking up
a
